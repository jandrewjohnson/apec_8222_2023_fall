{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this notebook we will learn how we could use a neural network to predict cancer based on medical images. \n",
                "\n",
                "#### Start by importing packages\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "source": [
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.datasets import make_moons, load_breast_cancer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from matplotlib import pyplot as plt"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the data\n",
                "\n",
                "Again we will use a dataset built-in to Sklearn that includes data related to diagnosing breast cancer."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "cancer = load_breast_cancer()\n",
                "\n",
                "# print('Dataset raw object', cancer)\n",
                "print('Dataset description', cancer['DESCR'])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Split into our training and testing XY sets"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 1: Scale the data\n",
                "\n",
                "The Multilayer Perceptron (MLP) approach is one of the few that doesn't automatically scale the data, so let's do that. Here we will use Numpy to do it manually, though there are alternative built-in methods within scikit-learn.\n",
                "\n",
                "In the code block below, use `X_train.mean(axis=0)` and similar functions to scale ALL of the X variables so that they have mean 0 and standard deviation 1. HINT: X_train and others are numpy arrays and so you can use fast raster math, e.g., `X_train - mean_on_train`.\n",
                "\n",
                "## Exercise 1 Answer"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Exercies 1 Code\n",
                "\n",
                "# Using numpy functions, compute the mean value per feature on the training set and the STD.\n",
                "# May want to remind ourselves what the X_train looks like.\n",
                "print('X_train', X_train)\n",
                "\n",
                "# Numpy arrays have a .mean() method attached to each array. \n",
                "# Below we use that, though note that we have to specify which axis we should calculate the mean on.\n",
                "# `axis=0` specifies that we want the mean of each column (which is how the separate variables are stored)\n",
                "\n",
                "mean_on_train = X_train.mean(axis=0)\n",
                "# print('mean_on_train', mean_on_train)\n",
                "\n",
                "\n",
                "# the .std() function is similarily powerful/fast.\n",
                "std_on_train = X_train.std(axis=0)\n",
                "# print('std_on_train', std_on_train)\n",
                "\n",
                "\n",
                "# Still using the Numpy awesomeness,\n",
                "# subtract the mean, and scale by inverse standard deviation,\n",
                "# making it  mean=0 and std=1\n",
                "X_train_scaled = (X_train - mean_on_train) / std_on_train\n",
                "X_test_scaled = (X_test - mean_on_train) / std_on_train"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create the MLP model object and fit it\n",
                "\n",
                "Using this new scaled training data, we are ready to define a Neural Net, Known here as a Multi-Layer-Perceptron (MLP) classifier. Because this next line hides away millions of other lines of code, you may want to explore it. In VS Code, you can navigate to a function's definition by placing your cursor in the function and press f-12. Try it in the cell below on the `MLPClassifier` code! The best documentation is often the code itself."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "mlp = MLPClassifier(random_state=0)\n",
                "\n",
                "# Now fit it with the scaled X and y TRAINING data.\n",
                "mlp.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(mlp)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Assess the fit\n",
                "\n",
                "Now we assess MLP's accuracy on the TRAINING and the TESTING data.\n",
                "\n",
                "Notice here also I'm introducing another convenient way of combining strings and numbers. The {:.2f} specifies a placeholder for a 2-digit representation of a floating point number. The Format method then places that floating point value into that placeholder."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "score_train = mlp.score(X_train_scaled, y_train)\n",
                "score_test = mlp.score(X_test_scaled, y_test)\n",
                "\n",
                "print(\"Accuracy on training set: {:.3f}\".format(score_train))\n",
                "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Plot the inputs and hidden layers of the neural net\n",
                "\n",
                "It can be hard perhaps to visualize what exaclty the neural net looks like (there is no coefficients table to simply look at). But here, it is small enough to actually visualize the coefficients within the network.\n",
                "\n",
                "Below, we plot the coeffs_ array to see it."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "plt.figure(figsize=(20, 5))\n",
                "plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\n",
                "plt.yticks(range(30), cancer.feature_names)\n",
                "plt.xlabel(\"Columns in weight matrix\")\n",
                "plt.ylabel(\"Input feature\")\n",
                "plt.colorbar()\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercies 5.1.2: Understanding which features matter most\n",
                "\n",
                "One of the massive challenges in Neural Nets is understanding why exactly it makes the predictions it does. Can you identify which input feature shows the largest positive effect on on cancer diagnosis?\n",
                "\n",
                "You probably can't make heads or tails of it. Let's create a greatly simplified version of our neural network to try to see if we can understand it.\n",
                "\n",
                "Specifically create a new `MLPClassifier` but this time make it have only a single hidden layer. Hint: use f-12 on the MLPClassifier code to see it's documentation and figure out what new input variable you sohuld specify when calling `mlp = MLPClassifier( .......  )`. Plot the output coefficients just like above. With only a single layer, the variables become somewhat more interpretable. \n",
                "\n",
                "Which variable now seems to have the largest positive impact?"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Excercise 5.1.2 workspace"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": [
        {
            "kernelspec": {
                "name": "python3",
                "language": "python",
                "display_name": "Python 3 (ipykernel)"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 4
}