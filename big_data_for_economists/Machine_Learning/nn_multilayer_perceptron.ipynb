{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this notebook we will learn how we could use a neural network to predict cancer based on medical images. \n",
                "\n",
                "### Start by importing packages\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "source": [
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.datasets import make_moons, load_breast_cancer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from matplotlib import pyplot as plt"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load the data\n",
                "\n",
                "Again we will use a dataset built-in to Sklearn that includes data related to diagnosing breast cancer."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "cancer = load_breast_cancer()\n",
                "\n",
                "# print('Dataset raw object', cancer)\n",
                "print('Dataset description', cancer['DESCR'])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Split into our training and testing XY sets"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 1: Scale the data\n",
                "\n",
                "The Multilayer Perceptron (MLP) approach is one of the few that doesn't automatically scale the data, so let's do that. Here we will use Numpy to do it manually, though there are alternative built-in methods within scikit-learn.\n",
                "\n",
                "In the code block below, use `X_train.mean(axis=0)` and similar functions to scale ALL of the X variables so that they have mean 0 and standard deviation 1. HINT: X_train and others are numpy arrays and so you can use fast raster math, e.g., `X_train - mean_on_train`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Exercies 1 Code\n",
                "\n",
                "# Using numpy functions, compute the mean value per feature on the training set and the STD."
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create the MLP model object and fit it\n",
                "\n",
                "Using this new scaled training data, we are ready to define a Neural Net, Known here as a Multi-Layer-Perceptron (MLP) classifier. Because this next line hides away millions of other lines of code, you may want to explore it. In VS Code, you can navigate to a function's definition by placing your cursor in the function and press f-12. Try it in the cell below on the `MLPClassifier` code! The best documentation is often the code itself."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "mlp = MLPClassifier(random_state=0)\n",
                "\n",
                "# Now fit it with the scaled X and y TRAINING data.\n",
                "mlp.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(mlp)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Assess the fit\n",
                "\n",
                "Now we assess MLP's accuracy on the TRAINING and the TESTING data.\n",
                "\n",
                "Notice here also I'm introducing another convenient way of combining strings and numbers. The {:.2f} specifies a placeholder for a 2-digit representation of a floating point number. The Format method then places that floating point value into that placeholder."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "score_train = mlp.score(X_train_scaled, y_train)\n",
                "score_test = mlp.score(X_test_scaled, y_test)\n",
                "\n",
                "print(\"Accuracy on training set: {:.3f}\".format(score_train))\n",
                "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Plot the inputs and hidden layers of the neural net\n",
                "\n",
                "It can be hard perhaps to visualize what exaclty the neural net looks like (there is no coefficients table to simply look at). But here, it is small enough to actually visualize the coefficients within the network.\n",
                "\n",
                "Below, we plot the coeffs_ array to see it."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "plt.figure(figsize=(20, 5))\n",
                "plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\n",
                "plt.yticks(range(30), cancer.feature_names)\n",
                "plt.xlabel(\"Columns in weight matrix\")\n",
                "plt.ylabel(\"Input feature\")\n",
                "plt.colorbar()\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercies 5.1.2: Understanding which features matter most\n",
                "\n",
                "One of the massive challenges in Neural Nets is understanding why exactly it makes the predictions it does. Can you identify which input feature shows the largest positive effect on on cancer diagnosis?\n",
                "\n",
                "You probably can't make heads or tails of it. Let's create a greatly simplified version of our neural network to try to see if we can understand it.\n",
                "\n",
                "Specifically create a new `MLPClassifier` but this time make it have only a single hidden layer. Hint: use f-12 on the MLPClassifier code to see it's documentation and figure out what new input variable you sohuld specify when calling `mlp = MLPClassifier( .......  )`. Plot the output coefficients just like above. With only a single layer, the variables become somewhat more interpretable. \n",
                "\n",
                "Which variable now seems to have the largest positive impact?"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Excercise 5.1.2 workspace"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Other approaches to understanding neural nets\n",
                "\n",
                "- Let's apply our new MLPClassifier model type to our hand-written digits dataset.\n",
                "  - Here, though, I want to highight a few other tools used in the ML workflow.\n",
                "    - make_pipeline\n",
                "    - StandardScaler"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import warnings\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.datasets import fetch_openml\n",
                "from sklearn.exceptions import ConvergenceWarning\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.neural_network import MLPClassifier"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use a fetch_openml helper function to get the MNIST dataset"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Load data from https://www.openml.org/d/554\n",
                "X, y = fetch_openml(\n",
                "    \"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\"\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scale the X variables"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "X = X / 255.0"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split data into train partition and test partition"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "print('Look at the data')\n",
                "print(X_train.shape, y_train.shape)\n",
                "print(X_train, y_train)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "mlp = MLPClassifier(\n",
                "    hidden_layer_sizes=(40,),\n",
                "    max_iter=8,\n",
                "    alpha=1e-4,\n",
                "    solver=\"sgd\",\n",
                "    verbose=10,\n",
                "    random_state=1,\n",
                "    learning_rate_init=0.2,\n",
                ")\n",
                "\n",
                "# this example won't converge because of resource usage constraints on\n",
                "# our Continuous Integration infrastructure, so we catch the warning and\n",
                "# ignore it here\n",
                "with warnings.catch_warnings():\n",
                "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
                "    mlp.fit(X_train, y_train)\n",
                "\n",
                "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
                "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "fig, axes = plt.subplots(4, 4)\n",
                "# use global min / max to ensure all weights are shown on the same scale\n",
                "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
                "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
                "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n",
                "    ax.set_xticks(())\n",
                "    ax.set_yticks(())\n",
                "\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import numpy as np\n",
                "from matplotlib import pyplot as plt\n",
                "from matplotlib.colors import ListedColormap\n",
                "\n",
                "from sklearn.datasets import make_circles, make_classification, make_moons\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.pipeline import make_pipeline\n",
                "from sklearn.preprocessing import StandardScaler"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "h = 0.02  # step size in the mesh\n",
                "\n",
                "alphas = np.logspace(-1, 1, 5)\n",
                "\n",
                "classifiers = []\n",
                "names = []\n",
                "for alpha in alphas:\n",
                "    classifiers.append(\n",
                "        make_pipeline(\n",
                "            StandardScaler(),\n",
                "            MLPClassifier(\n",
                "                solver=\"lbfgs\",\n",
                "                alpha=alpha,\n",
                "                random_state=1,\n",
                "                max_iter=2000,\n",
                "                early_stopping=True,\n",
                "                hidden_layer_sizes=[10, 10],\n",
                "            ),\n",
                "        )\n",
                "    )\n",
                "    names.append(f\"alpha {alpha:.2f}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "X, y = make_classification(\n",
                "    n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1\n",
                ")\n",
                "rng = np.random.RandomState(2)\n",
                "X += 2 * rng.uniform(size=X.shape)\n",
                "linearly_separable = (X, y)\n",
                "\n",
                "datasets = [\n",
                "    make_moons(noise=0.3, random_state=0),\n",
                "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
                "    linearly_separable,\n",
                "]\n",
                "\n",
                "figure = plt.figure(figsize=(17, 9))\n",
                "i = 1\n",
                "# iterate over datasets\n",
                "for X, y in datasets:\n",
                "    # split into training and test part\n",
                "    X_train, X_test, y_train, y_test = train_test_split(\n",
                "        X, y, test_size=0.4, random_state=42\n",
                "    )\n",
                "\n",
                "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
                "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
                "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
                "\n",
                "    # just plot the dataset first\n",
                "    cm = plt.cm.RdBu\n",
                "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
                "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
                "    # Plot the training points\n",
                "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
                "    # and testing points\n",
                "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
                "    ax.set_xlim(xx.min(), xx.max())\n",
                "    ax.set_ylim(yy.min(), yy.max())\n",
                "    ax.set_xticks(())\n",
                "    ax.set_yticks(())\n",
                "    i += 1\n",
                "\n",
                "    # iterate over classifiers\n",
                "    for name, clf in zip(names, classifiers):\n",
                "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
                "        clf.fit(X_train, y_train)\n",
                "        score = clf.score(X_test, y_test)\n",
                "\n",
                "        # Plot the decision boundary. For that, we will assign a color to each\n",
                "        # point in the mesh [x_min, x_max] x [y_min, y_max].\n",
                "        if hasattr(clf, \"decision_function\"):\n",
                "            Z = clf.decision_function(np.column_stack([xx.ravel(), yy.ravel()]))\n",
                "        else:\n",
                "            Z = clf.predict_proba(np.column_stack([xx.ravel(), yy.ravel()]))[:, 1]\n",
                "\n",
                "        # Put the result into a color plot\n",
                "        Z = Z.reshape(xx.shape)\n",
                "        ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\n",
                "\n",
                "        # Plot also the training points\n",
                "        ax.scatter(\n",
                "            X_train[:, 0],\n",
                "            X_train[:, 1],\n",
                "            c=y_train,\n",
                "            cmap=cm_bright,\n",
                "            edgecolors=\"black\",\n",
                "            s=25,\n",
                "        )\n",
                "        # and testing points\n",
                "        ax.scatter(\n",
                "            X_test[:, 0],\n",
                "            X_test[:, 1],\n",
                "            c=y_test,\n",
                "            cmap=cm_bright,\n",
                "            alpha=0.6,\n",
                "            edgecolors=\"black\",\n",
                "            s=25,\n",
                "        )\n",
                "\n",
                "        ax.set_xlim(xx.min(), xx.max())\n",
                "        ax.set_ylim(yy.min(), yy.max())\n",
                "        ax.set_xticks(())\n",
                "        ax.set_yticks(())\n",
                "        ax.set_title(name)\n",
                "        ax.text(\n",
                "            xx.max() - 0.3,\n",
                "            yy.min() + 0.3,\n",
                "            f\"{score:.3f}\".lstrip(\"0\"),\n",
                "            size=15,\n",
                "            horizontalalignment=\"right\",\n",
                "        )\n",
                "        i += 1\n",
                "\n",
                "figure.subplots_adjust(left=0.02, right=0.98)\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": [
        {
            "kernelspec": {
                "name": "python3",
                "language": "python",
                "display_name": "Python 3 (ipykernel)"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 4
}