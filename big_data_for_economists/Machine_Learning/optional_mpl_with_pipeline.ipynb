{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Needs documentation still but code works"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import numpy as np\n",
                "from matplotlib import pyplot as plt\n",
                "from matplotlib.colors import ListedColormap\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.datasets import make_moons, make_circles, make_classification\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.pipeline import make_pipeline\n",
                "\n",
                "# This activity compares different values for regularization parameter ‘alpha’.\n",
                "# The plot shows that different alphas yield different decision functions.\n",
                "\n",
                "# Recall that alpha is a parameter for regularization term, aka penalty term, that combats\n",
                "# overfitting by constraining the size of the weights. Increasing alpha may fix\n",
                "# high variance (a sign of overfitting) by encouraging smaller weights,\n",
                "# resulting in a decision boundary plot that appears with lesser curvatures.\n",
                "# Similarly, decreasing alpha may fix high bias (a sign of underfitting) by\n",
                "# encouraging larger weights, potentially resulting in a more complicated decision boundary.\n",
                "\n",
                "# Create a vector of alphas to test.\n",
                "alphas = np.logspace(-5, 3, 5)\n",
                "\n",
                "# Assign those alphas to some names.\n",
                "# Notice also a very cool feature in python called \"list comprehension\":\n",
                "\n",
                "# Instead of\n",
                "# for i in alphas:\n",
                "#   names.append(alpha ' + str(i))\n",
                "\n",
                "# List comprehension defines the for loop INSIDE a list.\n",
                "names = ['alpha ' + str(i) for i in alphas]\n",
                "\n",
                "# print('names', names)\n",
                "\n",
                "# Now for the heavy lifting\n",
                "# We will create a Pipeline of transforms with a final estimator.\n",
                "\n",
                "# Sequentially apply a list of transforms and a final estimator.\n",
                "# Intermediate steps of the pipeline must be ‘transforms’, that is,\n",
                "# they must implement fit and transform methods. The final estimator only\n",
                "# needs to implement fit.\n",
                "\n",
                "# The purpose of the pipeline is to assemble several steps that can be cross-validated\n",
                "# together while setting different parameters.\n",
                "\n",
                "classifiers = []\n",
                "for i in alphas:\n",
                "\n",
                "    # Assign a classifier into the pipeline along with a scaler object.\n",
                "    classifiers.append(make_pipeline(\n",
                "                       StandardScaler(),\n",
                "                       MLPClassifier(solver='lbfgs', alpha=i,\n",
                "                                     random_state=1, max_iter=2000,\n",
                "                                     early_stopping=True,\n",
                "                                     hidden_layer_sizes=[100, 100])\n",
                "                       ))\n",
                "\n",
                "# Use one of sklearn's built-in data generators to generate some 2 dimensional (2 feature)\n",
                "# Data to classify.\n",
                "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
                "                           random_state=0, n_clusters_per_class=1)\n",
                "rng = np.random.RandomState(2)\n",
                "X += 2 * rng.uniform(size=X.shape)\n",
                "linearly_separable = (X, y)\n",
                "\n",
                "# this generates the following three datasets that each pose unique challenges for classification\n",
                "datasets = [make_moons(noise=0.3, random_state=0),\n",
                "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
                "            linearly_separable]\n",
                "\n",
                "\n",
                "# Take a look at the results to see different classifications, along with how they scale with\n",
                "# the regularization parameter.\n",
                "figure = plt.figure(figsize=(17, 9))\n",
                "i = 1\n",
                "\n",
                "h = .02  # step size in the mesh (the thing we'll actually plot).\n",
                "# iterate over datasets\n",
                "for X, y in datasets:\n",
                "    # preprocess dataset, split into training and test part\n",
                "    X = StandardScaler().fit_transform(X)\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
                "\n",
                "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
                "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
                "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
                "                         np.arange(y_min, y_max, h))\n",
                "\n",
                "    # just plot the dataset first\n",
                "    cm = plt.cm.RdBu\n",
                "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
                "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
                "    # Plot the training points\n",
                "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
                "    # and testing points\n",
                "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
                "    ax.set_xlim(xx.min(), xx.max())\n",
                "    ax.set_ylim(yy.min(), yy.max())\n",
                "    ax.set_xticks(())\n",
                "    ax.set_yticks(())\n",
                "    i += 1\n",
                "\n",
                "    # iterate over classifiers\n",
                "    for name, clf in zip(names, classifiers):\n",
                "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
                "        clf.fit(X_train, y_train)\n",
                "        score = clf.score(X_test, y_test)\n",
                "\n",
                "        # Plot the decision boundary. For that, we will assign a color to each\n",
                "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "        if hasattr(clf, \"decision_function\"):\n",
                "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
                "        else:\n",
                "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
                "\n",
                "        # Put the result into a color plot\n",
                "        Z = Z.reshape(xx.shape)\n",
                "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
                "\n",
                "        # Plot also the training points\n",
                "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
                "                   edgecolors='black', s=25)\n",
                "        # and testing points\n",
                "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
                "                   alpha=0.6, edgecolors='black', s=25)\n",
                "\n",
                "        ax.set_xlim(xx.min(), xx.max())\n",
                "        ax.set_ylim(yy.min(), yy.max())\n",
                "        ax.set_xticks(())\n",
                "        ax.set_yticks(())\n",
                "        ax.set_title(name)\n",
                "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
                "                size=15, horizontalalignment='right')\n",
                "        i += 1\n",
                "\n",
                "figure.subplots_adjust(left=.02, right=.98)\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": [
        {
            "kernelspec": {
                "name": "python3",
                "language": "python",
                "display_name": "Python 3 (ipykernel)"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 4
}